
\begin{table}[t]
    \caption{Results showing how \boldmath{$\lambda$} affected the performance of the re-ranked list of results, using a set of queries issued by subjects from Maxwell et al.~\cite{maxwell2017snippet_length}. Here, $k=30$.}
    \label{tbl:previous_queries}
    \renewcommand{\arraystretch}{1.4}
    \begin{center}
    \begin{tabulary}{\textwidth}{L{0.6cm}||D{0.87cm}|D{0.87cm}|D{0.87cm}|D{0.87cm}|D{0.87cm}|D{0.87cm}}
    \hline
    
    % HEADERS
    & \multicolumn{2}{c|}{\boldmath{$\alpha$}\textbf{DCG}} & \multicolumn{2}{c|}{\textbf{\# Aspects}} & \multicolumn{2}{c}{\boldmath{$P@k$}} \\
    \boldmath{$\lambda$} & \boldmath{$@5$} & \boldmath{$@10$} & \boldmath{$@5$} & \boldmath{$@10$} & \boldmath{$@5$} & \boldmath{$@10$} \\ \hline\hline
    
    % VALUES
    \boldmath{$0.0$} & \todo{x} & \todo{x} & \todo{x} & \todo{x} & \todo{x} \\ \hline
    \boldmath{$0.1$} & \todo{x} & \todo{x} & \todo{x} & \todo{x} & \todo{x} \\ \hline
    \boldmath{$0.5$} & \todo{x} & \todo{x} & \todo{x} & \todo{x} & \todo{x} \\ \hline
    \boldmath{$0.7$} & \todo{x} & \todo{x} & \todo{x} & \todo{x} & \todo{x} \\ \hline
    \boldmath{$1.0$} & \todo{x} & \todo{x} & \todo{x} & \todo{x} & \todo{x} \\ \hline
    % END
\end{tabulary}
\end{center}
\end{table}

\begin{table}[t]
    \caption{Demonstration of the diversification algorithm's performance when utilising only entities found within TREC relevant documents \emph{(RELS)}, as well as both TREC relevant (judgement \boldmath{$>=1$}) and TREC non-relevant (judgement \boldmath{$=0$}) \emph{(BOTH)}. Here, $k=30$ and $\lambda=0.7$.}
    \label{tbl:previous_queries_set}
    \renewcommand{\arraystretch}{1.4}
    \begin{center}
    \begin{tabulary}{\textwidth}{L{0.6cm}||D{0.87cm}|D{0.87cm}|D{0.87cm}|D{0.87cm}|D{0.87cm}|D{0.87cm}}
    \hline
    
    % HEADERS
    & \multicolumn{2}{c|}{\boldmath{$\alpha$}\textbf{DCG}} & \multicolumn{2}{c|}{\textbf{\# Aspects}} & \multicolumn{2}{c}{\boldmath{$P@k$}} \\
    \textbf{Set} & \boldmath{$@5$} & \boldmath{$@10$} & \boldmath{$@5$} & \boldmath{$@10$} & \boldmath{$@5$} & \boldmath{$@10$} \\ \hline\hline
    
    % VALUES
    \textbf{R} & \todo{x} & \todo{x} & \todo{x} & \todo{x} & \todo{x} \\ \hline
    \textbf{R+N} & \todo{x} & \todo{x} & \todo{x} & \todo{x} & \todo{x} \\ \hline
    % END
\end{tabulary}
\end{center}
\end{table}



\begin{algorithm}[t!]
    \textbf{Inputs:} original ranking \texttt{existing\_results}; \emph{(ii)} diversification depth, $k$; \emph{(iii)} $\lambda$, weighting for diversification scoring component\\
    \textbf{Output:} manipulated set of results, diversified to rank $k$
    \BlankLine
    
    \texttt{entities} $\longleftarrow$ []\\
    \texttt{new\_rankings} $\longleftarrow$ []\\
    \texttt{i} $\longleftarrow$ $1$\\
    
    \BlankLine
    
    \texttt{pop()} top ranked document from \texttt{existing\_results} into \texttt{new\_rankings}\\
    
    \BlankLine
    
    \While{$i <= k$} {
        \texttt{entities} $\longleftarrow$ all entities in \texttt{new\_rankings} from $0$ to \texttt{i}$-1$\\
        \texttt{j}$\longleftarrow 0$\\
        
        \BlankLine
        
        \While{$j <=$ length of \texttt{existing\_results}} {
            \texttt{new\_entity\_count} $\longleftarrow$ the number of entities in document that are not in \texttt{entities} (unseen in the previous rankings)
            
            \BlankLine
            
            \texttt{existing\_results}[$j$]\texttt{.score} $\longleftarrow$ score + $(\lambda \cdot$ \texttt{new\_entity\_count}$)$
            
            \BlankLine
            
            increment \texttt{j}\\
        }
        
        \BlankLine
        
        sort \texttt{existing\_results} by \texttt{score}\\
        \texttt{pop()} top ranked document from \texttt{existing\_results} into \texttt{new\_rankings}\\
        
        \BlankLine
        
        increment \texttt{i}\\
    }
    
    \BlankLine
    \Return{\texttt{new\_rankings}}
    
    \vspace*{4mm}
    \caption{Pseudo-code of the diversification algorithm used in this study, based upon the XQuAD framework by Santos et al.~\cite{santos2010query_reformulations_diversification}.}
    \label{alg:diversifying}
\end{algorithm}


%%%%%%%%
%\subsection{\todo{Diversifying Algorithm}}\label{sec:method:diversification}
%%%%%%%%
%In order to diversify our results, we were required to re-rank the documents provided by our system's initial BM25 ranking with respect to the subject's query. Our algorithm, based upon the XQuAD framework by Santos et al.~\cite{santos2010query_reformulations_diversification}, re-scores and subsequently re-ranks documents based upon the number on unseen entities that appear within the document. The algorithm is presented in Algorithm~\ref{alg:diversifying}. Essentially, documents are re-ranked according to the number of new entities that are contained within them, with $\lambda$ determining the weighting of the aspectual scoring component.


%In order to select a reasonable approximation for the algorithm's $\lambda$ weighting, we ran our diversification algorithm over the set of queries that were issued by subjects that partook in the study by Maxwell et al.~\cite{maxwell2017snippet_length}. We assumed that re-ranking would take place over the first 30 documents of each query, as subjects in this previously didn't go lower than \todo{$xx$} documents on average. Re-ranking the first 30 documents was also not particularly computationally intensive, ensuring the implicit contract between the user and the search engine of a quick response was not violated. We picked $\lambda=0.7$ for the study, as it offered good performance \todo{without doing too well.}

%For the diversity re-ranking to work in this scenario, the algorithm must be aware of the ground truths which were collated as part of this study (refer to Section~\ref{sec:method:entities}). The reasons for following this approach were: \emph{(i)} us not having to invest a significant amount of effort tuning a different diversification algorithm to return acceptable results; and \emph{(ii)} that it would guarantee that TREC relevant documents, containing different entities would bubble up to the top of the rankings, increasing the effect (and hopefully subject observation) that the results are indeed ranked differently. Without such a ground-truth based approach, ensuring that TREC relevant documents would bubble up would be difficult. The effects of document pooling restrict how many documents were assessed by the TREC assessors -- it is highly likely that many other documents exist within the collection that could be relevant to a given topic, but were not assessed.

%One pitfall of this approach is that the algorithm would become too perfect. With access to the ground truths, it would subsequently proceed to assign a higher rank to documents that were TREC assessed, leading to every document in the top $k$ containing new entities. To make this more of a challenge for subjects, we also incorporated within our aspectual data described in Section~\ref{sec:method:entities} documents that were considered to be non-relevant by TREC assessors (i.e. a TREC assessment of $0$). This led to including an additional $2,663$ documents that would be included in the diversity re-ranking, but are not relevant ($580$ for \textnumero~341, $500$ for \textnumero~347, $526$ for \textnumero~367, $502$ for \textnumero~408, and $555$ for \textnumero~435.). Rather than manually assess each document, we took the list of entities that were discovered for the TREC relevant documents, and performed an exact keyword search for each of the entities within each of the $2,663$ documents. Any matches would have the corresponding entity attached to the document. To illustrate the difference in performance, Table~\ref{tbl:previous_queries_set} reports various metrics, compared over the set of entities found within TREC relevant documents only, as well TREC relevant and TREC non-relevant.

%%%%%%%%
%For ad-hoc retrieval, subjects were requested to find at least \emph{four} documents that they considered to be relevant to the topic description provided. For aspectual retrieval tasks, subjects needed to find at least four documents that were relevant to the topic description \emph{and} contained new, unseen aspects about the topic (e.g. a new tropical storm for topic \textnumero~408). Unlike other similar studies, all search tasks were untimed -- searchers would decide when they wished to end the task. We selected four documents as the minimum target as subjects who undertook the study by Maxwell et al~\cite{maxwell2017snippet_length}spent approximately 10 minutes per search task to reach this target. As such, the study was designed such that subjects would spend approximately one hour searching and answering surveys.

%Each topic description, provided as part of the TREC 2005 Robust Track, was read and a salient aspect was extracted. With this single aspect, we could then instruct subjects to find documents satisfying the relevance criteria of the task, while at the same time discussing new, unseen entities for the given aspect. For each topic, we selected the following aspects for searchers to consider.

%As part of the TREC Robust Track, a series of relevance judgements (in the form of \emph{TREC QRELs}) are provided. While suitable for prior studies employing ad-hoc retrieval, where only a relevance judgement is required, this data were insufficient for the aspectual retrieval tasks of this study. Here, we required information on what \emph{entities} are discussed within the different documents of the TREC AQUAINT collection, in order to determine how well subjects performed in the aspectual retrieval tasks.

%With such data unavailable in the public domain (to the best of our knowledge), we examined documents from the TREC AQUAINT collection, determining what entities were discussed in each. This process was undertaken manually to ensure reliability of the gathered entities. As the TREC AQUAINT collection contains over one million news articles, this was unfeasible -- instead, we examined only the documents that were judged as TREC relevant (with a judgement of either $1$ or $2$) for each of the five topics we consider in this study. This reduced the number of documents to examine to $632$ documents in total.

